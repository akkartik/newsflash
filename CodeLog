=-=1 - Wed Jan 14 18:32:20 PST 2009
Simple URL download.
=-=2 - Sat Jan 17 12:01:03 PST 2009
Download all URLs.
=-=3 - Sat Jan 17 13:39:41 PST 2009
No need for a download step. Just use feedparser.
=-=4 - Sat Jan 17 14:00:45 PST 2009
Setup task to import feeds.
=-=5 - Sat Jan 17 15:40:19 PST 2009
Import feeds done. Now let's get it working on slicehost repo.
=-=6 - Mon Jan 19 20:35:19 PST 2009
Done.
=-=7 - Tue Jan 20 09:23:44 PST 2009

Remove id field.
=-=8 - Tue Jan 20 10:05:54 PST 2009
Turns out we do need id field for yaml fixtures.
=-=9 - Tue Jan 20 10:42:30 PST 2009

Bugfix: get contents from crawled feeds.
=-=10 - Tue Jan 20 10:47:36 PST 2009

Hyp: "execution expired' is because of Timeout::Error:
  http://www.slashdotdash.net/2008/02/15/ruby-tidbit-timeout-code-execution
=-=11 - Tue Jan 20 18:31:32 PST 2009
=-=12 - Tue Jan 20 18:35:16 PST 2009

Feed crawler still not working properly - title and content of all feeds is
borked.
It's becoming clear that's where all the complexity is. The front-end is easy.
=-=15 - Wed Jan 21 10:52:54 PST 2009
Turns out most of the misbehaving feeds are atom - we aren't handling atom
feeds properly.
Some others have content but seemingly no permalink.
I did this triage on the air.
=-=16 - Wed Jan 21 19:12:17 PST 2009
=-=17 - Wed Jan 21 19:12:39 PST 2009
Now we know how to handle atom feeds.
=-=18 - Wed Jan 21 20:05:24 PST 2009
Descriptions are sometimes not getting parsed right. File encoding issues,
perhaps.
=-=19 - Wed Jan 21 20:42:54 PST 2009
Cleanup.
=-=20 - Wed Jan 21 20:52:27 PST 2009
=-=21 - Wed Jan 21 21:03:10 PST 2009
Reverse the feed entries - that way we only need to sort by id descending
within a feed, and diversity means we don't care about parsing times,
timezones, any of that crap.
=-=22 - Thu Jan 22 07:22:30 PST 2009
Feed crawler daemonified.
=-=23 - Thu Jan 22 07:37:42 PST 2009
Standardized feed file location between slicehost and air.
=-=24 - Thu Jan 22 07:56:10 PST 2009

First-cut implementation for the workhorse homepage. No update yet, all it
does is show most recent post from first feed.
Already we find an error in our db schema and crawler.
=-=25 - Thu Jan 22 08:24:05 PST 2009
Back-end done for AJAX update. Not properly connected up with view yet.
=-=26 - Thu Jan 22 10:08:38 PST 2009
View connected up, but not updated yet. Should I really be using show for the
remote_link? Shouldn't it be update? Also need to set doneReading, and skip
posts we're doneReading, and retry next feedurl if this one has nothing.
=-=27 - Thu Jan 22 10:14:59 PST 2009
Add fixture for feed with all read posts as a first step.
=-=28 - Thu Jan 22 10:18:32 PST 2009
First cut at support for doneReading.
=-=29 - Thu Jan 22 10:29:19 PST 2009
doneReading done. most_recent_from_next_feed is ugly, though.
=-=30 - Thu Jan 22 10:34:51 PST 2009
Clean.
=-=31 - Thu Jan 22 10:40:02 PST 2009

Crawler. I need a way to detect illegal junk that's going to throw off sql.
Maybe sql-escape and unescape.
=-=32 - Thu Jan 22 10:46:34 PST 2009
Nope, that's not it.
Done with one iteration of the crawler.
=-=33 - Thu Jan 22 19:36:39 PST 2009

Done with the update link to go to the next post. But the browser seems to
have trouble with repeated Element.replace on the same object. Is it the fact
that the id is actually changing itself? Let's try replace_html instead.
=-=34 - Fri Jan 23 07:29:31 PST 2009
Done, but still not working.
=-=35 - Fri Jan 23 07:33:47 PST 2009
It's a property of certain feeds!! When the next feed is raganwald's the
update doesn't happen. WTF!?
Also found that I did indeed need to strip surrounding quotes from rfeedparser
output.
=-=36 - Fri Jan 23 07:40:34 PST 2009
Ah, it was quoting after all. Quotes in the URL confuse the browser.
=-=37 - Fri Jan 23 07:45:42 PST 2009

First call may need to skip empty feeds as well.
I'm making sure title and blog title aren't empty strings so we have something
to click on. After the next crawl we can remove that code from the controller.
=-=38 - Fri Jan 23 09:52:21 PST 2009
=-=39 - Fri Jan 23 10:25:42 PST 2009
=-=40 - Fri Jan 23 12:50:17 PST 2009
