=-=1 - Wed Jan 14 18:32:20 PST 2009
Simple URL download.
=-=2 - Sat Jan 17 12:01:03 PST 2009
Download all URLs.
=-=3 - Sat Jan 17 13:39:41 PST 2009
No need for a download step. Just use feedparser.
=-=4 - Sat Jan 17 14:00:45 PST 2009
Setup task to import feeds.
=-=5 - Sat Jan 17 15:40:19 PST 2009
Import feeds done. Now let's get it working on slicehost repo.
=-=6 - Mon Jan 19 20:35:19 PST 2009
Done.
=-=7 - Tue Jan 20 09:23:44 PST 2009

Remove id field.
=-=8 - Tue Jan 20 10:05:54 PST 2009
Turns out we do need id field for yaml fixtures.
=-=9 - Tue Jan 20 10:42:30 PST 2009

Bugfix: get contents from crawled feeds.
=-=10 - Tue Jan 20 10:47:36 PST 2009

Hyp: "execution expired' is because of Timeout::Error:
  http://www.slashdotdash.net/2008/02/15/ruby-tidbit-timeout-code-execution
=-=11 - Tue Jan 20 18:31:32 PST 2009
=-=12 - Tue Jan 20 18:35:16 PST 2009

Feed crawler still not working properly - title and content of all feeds is
borked.
It's becoming clear that's where all the complexity is. The front-end is easy.
=-=15 - Wed Jan 21 10:52:54 PST 2009
Turns out most of the misbehaving feeds are atom - we aren't handling atom
feeds properly.
Some others have content but seemingly no permalink.
I did this triage on the air.
=-=16 - Wed Jan 21 19:12:17 PST 2009
=-=17 - Wed Jan 21 19:12:39 PST 2009
Now we know how to handle atom feeds.
=-=18 - Wed Jan 21 20:05:24 PST 2009
Descriptions are sometimes not getting parsed right. File encoding issues,
perhaps.
=-=19 - Wed Jan 21 20:42:54 PST 2009
Cleanup.
=-=20 - Wed Jan 21 20:52:27 PST 2009
=-=21 - Wed Jan 21 21:03:10 PST 2009
Reverse the feed entries - that way we only need to sort by id descending
within a feed, and diversity means we don't care about parsing times,
timezones, any of that crap.
=-=22 - Thu Jan 22 07:22:30 PST 2009
Feed crawler daemonified.
=-=23 - Thu Jan 22 07:37:42 PST 2009
Standardized feed file location between slicehost and air.
=-=24 - Thu Jan 22 07:56:10 PST 2009
